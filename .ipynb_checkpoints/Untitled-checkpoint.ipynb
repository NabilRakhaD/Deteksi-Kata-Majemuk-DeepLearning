{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for preprocessing\n",
    "max_length = max([len(word) for word in data['word']]) # Maximum length of a word\n",
    "vocab_size = len(set(''.join(data['word']))) # Number of unique characters\n",
    "embedding_dim = 50 # Size of the embedding vector\n",
    "\n",
    "# Tokenizing the characters in the words\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data['word'])\n",
    "sequences = tokenizer.texts_to_sequences(data['word'])\n",
    "\n",
    "# Padding sequences\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encoding labels (1 for correct, 0 for incorrect)\n",
    "y = np.where(data['label'] == 'correct', 1, 0)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(Dropout(0.5))  # Dropout layer\n",
    "model.add(Bidirectional(LSTM(64)))  # Bidirectional LSTM\n",
    "model.add(Dropout(0.5))  # Another dropout layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Callbacks for early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# Train the model with callbacks\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83f792df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset kata benar dan typo.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de132c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0561f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000241C8191EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000241C8191EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 297ms/step\n",
      "Word: aku, Correct: True\n",
      "Word: ingin, Correct: True\n",
      "Word: bicara, Correct: True\n",
      "Word: dengan, Correct: True\n",
      "Word: kamu, Correct: True\n",
      "Word: kkamu, Correct: False\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the model\n",
    "model = load_model('first_model.h5')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f89cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Word: ia, Correct: False\n",
      "Word: mengatakan, Correct: True\n",
      "Word: hal, Correct: True\n",
      "Word: itu, Correct: True\n",
      "Word: antara, Correct: True\n",
      "Word: lain, Correct: False\n",
      "Word: karena, Correct: True\n",
      "Word: anak-anak, Correct: True\n",
      "Word: sd, Correct: False\n",
      "Word: yang, Correct: True\n",
      "Word: kehilangan, Correct: True\n",
      "Word: masa, Correct: True\n",
      "Word: belajar, Correct: True\n",
      "Word: selama, Correct: True\n",
      "Word: 6, Correct: True\n",
      "Word: bulan, Correct: True\n",
      "Word: sama, Correct: True\n",
      "Word: dengan, Correct: True\n",
      "Word: kehilangan, Correct: True\n",
      "Word: 2, Correct: True\n",
      "Word: tahun, Correct: True\n",
      "Word: pengalaman, Correct: True\n",
      "Word: belajarnya, Correct: True\n",
      "Word: berdasarkan, Correct: True\n",
      "Word: riset, Correct: True\n",
      "Word: yang, Correct: True\n",
      "Word: pernah, Correct: False\n",
      "Word: dibacanya, Correct: True\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# Assuming 'tokenizer' is your character-level tokenizer used during training\n",
    "def preprocess_sentence(sentence, tokenizer, max_length):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation.replace('-', '')))\n",
    "    words = sentence.split()\n",
    "    tokenized_words = tokenizer.texts_to_sequences(words)\n",
    "    padded_words = pad_sequences(tokenized_words, maxlen=max_length, padding='post')\n",
    "    return words, padded_words\n",
    "\n",
    "sentence = 'Ia mengatakan hal itu antara lain karena anak-anak SD yang kehilangan masa belajar selama 6 bulan sama dengan kehilangan 2 tahun pengalaman belajarnya berdasarkan riset yang pernah dibacanya.'\n",
    "words, preprocessed_sentence = preprocess_sentence(sentence, tokenizer, max_length)\n",
    "\n",
    "predictions = model.predict(preprocessed_sentence)\n",
    "\n",
    "# Determine a threshold for correctness, e.g., 0.5\n",
    "threshold = 0.35\n",
    "correctness = predictions > threshold\n",
    "\n",
    "for word, is_correct in zip(words, correctness):\n",
    "    print(f\"Word: {word}, Correct: {is_correct[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4605b4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55805874],\n",
       "       [0.60184956],\n",
       "       [0.62456554],\n",
       "       [0.31416276],\n",
       "       [0.51999176],\n",
       "       [0.7241107 ],\n",
       "       [0.55941284],\n",
       "       [0.00215949],\n",
       "       [0.54350924],\n",
       "       [0.34861234],\n",
       "       [0.00636769],\n",
       "       [0.28181365],\n",
       "       [0.19518377],\n",
       "       [0.69913644],\n",
       "       [0.9074866 ],\n",
       "       [0.4565683 ],\n",
       "       [0.6757054 ],\n",
       "       [0.59095037],\n",
       "       [0.55003476],\n",
       "       [0.66215694],\n",
       "       [0.381959  ],\n",
       "       [0.43490946],\n",
       "       [0.5821152 ],\n",
       "       [0.7402198 ],\n",
       "       [0.67783976],\n",
       "       [0.5810329 ],\n",
       "       [0.62728345],\n",
       "       [0.7248004 ],\n",
       "       [0.38043228],\n",
       "       [0.8226982 ],\n",
       "       [0.9074866 ],\n",
       "       [0.78651905],\n",
       "       [0.38902643],\n",
       "       [0.56354904],\n",
       "       [0.26841888],\n",
       "       [0.66474503],\n",
       "       [0.46698517]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c788fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 32, 50)            1550      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 50)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              58880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,559\n",
      "Trainable params: 60,559\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "6820/6820 [==============================] - 154s 22ms/step - loss: 0.4344 - accuracy: 0.8019 - val_loss: 0.3666 - val_accuracy: 0.8247\n",
      "Epoch 2/10\n",
      "6820/6820 [==============================] - 146s 21ms/step - loss: 0.3633 - accuracy: 0.8235 - val_loss: 0.3333 - val_accuracy: 0.8324\n",
      "Epoch 3/10\n",
      "6820/6820 [==============================] - 149s 22ms/step - loss: 0.3420 - accuracy: 0.8296 - val_loss: 0.3197 - val_accuracy: 0.8408\n",
      "Epoch 4/10\n",
      "6820/6820 [==============================] - 90s 13ms/step - loss: 0.3323 - accuracy: 0.8329 - val_loss: 0.3133 - val_accuracy: 0.8415\n",
      "Epoch 5/10\n",
      "6820/6820 [==============================] - 165s 24ms/step - loss: 0.3243 - accuracy: 0.8360 - val_loss: 0.3059 - val_accuracy: 0.8444\n",
      "Epoch 6/10\n",
      "6820/6820 [==============================] - 146s 21ms/step - loss: 0.3187 - accuracy: 0.8388 - val_loss: 0.3018 - val_accuracy: 0.8460\n",
      "Epoch 7/10\n",
      "6820/6820 [==============================] - 88s 13ms/step - loss: 0.3141 - accuracy: 0.8413 - val_loss: 0.2962 - val_accuracy: 0.8479\n",
      "Epoch 8/10\n",
      "6820/6820 [==============================] - 93s 14ms/step - loss: 0.3109 - accuracy: 0.8425 - val_loss: 0.2951 - val_accuracy: 0.8497\n",
      "Epoch 9/10\n",
      "6820/6820 [==============================] - 154s 23ms/step - loss: 0.3070 - accuracy: 0.8439 - val_loss: 0.2885 - val_accuracy: 0.8530\n",
      "Epoch 10/10\n",
      "6820/6820 [==============================] - 175s 26ms/step - loss: 0.3047 - accuracy: 0.8448 - val_loss: 0.2897 - val_accuracy: 0.8519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241c84cbbb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'path_to_your_dataset.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Parameters for preprocessing\n",
    "max_length = max([len(word) for word in data['word']]) # Maximum length of a word\n",
    "vocab_size = len(set(''.join(data['word']))) # Number of unique characters\n",
    "embedding_dim = 50 # Size of the embedding vector\n",
    "\n",
    "# Tokenizing the characters in the words\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data['word'])\n",
    "sequences = tokenizer.texts_to_sequences(data['word'])\n",
    "\n",
    "# Padding sequences\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encoding labels (1 for correct, 0 for incorrect)\n",
    "y = np.where(data['label'] == 'correct', 1, 0)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(64)) # You can adjust the number of LSTM units\n",
    "model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val)) # Adjust epochs as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e248850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'path_to_your_dataset.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Parameters for preprocessing\n",
    "max_length = max([len(word) for word in data['word']]) # Maximum length of a word\n",
    "vocab_size = len(set(''.join(data['word']))) # Number of unique characters\n",
    "embedding_dim = 100 # Size of the embedding vector\n",
    "\n",
    "# Tokenizing the characters in the words\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data['word'])\n",
    "sequences = tokenizer.texts_to_sequences(data['word'])\n",
    "\n",
    "# Padding sequences\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encoding labels (1 for correct, 0 for incorrect)\n",
    "y = np.where(data['label'] == 'correct', 1, 0)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Bidirectional LSTM\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Bidirectional(LSTM(64)))  # Another Bidirectional LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fce595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d8bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "\n",
    "def generate_typo(word):\n",
    "    \n",
    "    # generate typo tipe 1: penggandaan huruf secara acak\n",
    "    if len(word) >= 1:\n",
    "        # pilih indeks huruf yang akan digandakan secara acak\n",
    "        rand_idx = random.randint(0, len(word) - 1)\n",
    "        \n",
    "        # selama idx mengarah ke tanda strip maka idx random akan di generate terus\n",
    "        while word[rand_idx] == '-':\n",
    "            rand_idx = random.randint(0, len(word) - 1)\n",
    "\n",
    "        # masukkan indeks yang dipilih kedalam variabel letter\n",
    "        letter = word[rand_idx]\n",
    "\n",
    "        # kembalikan kata dengan huruf yang telah digandakan\n",
    "        typo1 = word[:rand_idx + 1] + letter + word[rand_idx + 1:]\n",
    "    else:\n",
    "        typo1 = None\n",
    "\n",
    "    \n",
    "    # generate typo tipe 2: menukar posisi 2 huruf secara acak\n",
    "    if len(word) >= 3:\n",
    "        # generate satu bilangan acak antara 1 dan panjang kata dikurangi 1\n",
    "        rand_idx1 = random.randint(1, len(word) - 1)\n",
    "        \n",
    "        # selama index berada di posisi terakhir kata, menuju ke tanda strip atau berada 1 posisi di belakang tanda strip\n",
    "        while rand_idx1 == len(word) - 1 or word[rand_idx1] == '-' or word[rand_idx1 + 1] == '-':\n",
    "            rand_idx1 = random.randint(1, len(word) - 1)\n",
    "        \n",
    "        rand_idx2 = rand_idx1 + 1\n",
    "\n",
    "        # menukar posisi dari 2 huruf berdasarkan index yang telah dihasilkan\n",
    "        typo2 = word[:rand_idx1] + word[rand_idx2] + word[rand_idx1] + word[rand_idx2 + 1:]\n",
    "    else:\n",
    "        typo2 = None\n",
    "\n",
    "    # generate typo tipe 3: menghapus satu huruf secara acak\n",
    "    if len(word) >= 3:\n",
    "        # generate random idx\n",
    "        rand_idx = random.randint(1, len(word) - 1)\n",
    "        \n",
    "        # jika index menuju ke tanda strip\n",
    "        while word[rand_idx] == '-':\n",
    "            rand_idx = random.randint(1, len(word) - 1)\n",
    "            \n",
    "        # hapus huruf pada indeks yang dihasilkan secara acak\n",
    "        typo3 = word[:rand_idx] + word[rand_idx+1:]\n",
    "    else:\n",
    "        typo3 = None\n",
    "\n",
    "    \n",
    "    # generate typo tipe 4: mengganti satu huruf dengan huruf lain secara acak\n",
    "    if len(word) >= 2:\n",
    "        # memilih indeks acak pada kata, kecuali indeks pertama\n",
    "        selected_index = random.randint(1, len(word) - 1)\n",
    "        \n",
    "        while word[selected_index] == '-':\n",
    "            selected_index = random.randint(1, len(word) - 1)\n",
    "\n",
    "        # memilih huruf acak untuk typo\n",
    "        typo_char = chr(random.randint(97, 122)) # karakter huruf kecil ASCII antara a dan z\n",
    "\n",
    "        # memeriksa apakah huruf acak sama dengan huruf pada indeks yang akan diganti\n",
    "        while typo_char == word[selected_index]:\n",
    "            typo_char = chr(random.randint(97, 122))\n",
    "\n",
    "        # mengganti karakter pada indeks yang dipilih dengan huruf typo\n",
    "        typo4 = word[:selected_index] + typo_char + word[selected_index + 1:]\n",
    "    else:\n",
    "        typo4 = None\n",
    "        \n",
    "    # membuat list kosong dengan nama generatedTypos\n",
    "    generatedTypos = []\n",
    "    \n",
    "    # memasukkan typo1, typo2, typo3, dan typo4 kedalam list jika tidak none\n",
    "    if typo1 is not None:\n",
    "        generatedTypos.append(typo1)\n",
    "    if typo2 is not None:\n",
    "        generatedTypos.append(typo2)\n",
    "    if typo3 is not None:\n",
    "        generatedTypos.append(typo3)\n",
    "    if typo4 is not None:\n",
    "        generatedTypos.append(typo4)\n",
    "        \n",
    "    # return list generatedTypos\n",
    "    return generatedTypos\n",
    "\n",
    "# membaca data dari file dataset kata benar.csv\n",
    "with open('dataset kata benar.csv', 'r') as wordData:\n",
    "    reader = csv.reader(wordData)\n",
    "    next(reader) # melewatkan baris pertama\n",
    "    data = list(reader)\n",
    "\n",
    "# melakukan generate typo pada setiap kata\n",
    "typos = []\n",
    "for row in data:\n",
    "    word = row[0]\n",
    "    if len(word) != 2:\n",
    "        typo_list = generate_typo(word)\n",
    "    \n",
    "    # membuat list berisi typo dan label incorrect\n",
    "    for typo in typo_list:\n",
    "        typo_data = [typo, \"incorrect\"]\n",
    "        typos.append(typo_data)\n",
    "\n",
    "\n",
    "# menuliskan hasil typo ke dalam file baru\n",
    "with open('dataset kata typo.csv', 'w', newline='') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(typos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec891d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('dataset kata typo.csv', header=None, names=['word', 'label'])\n",
    "df2 = pd.read_csv('dataset kata benar.csv')\n",
    "\n",
    "df = pd.concat([df2,df1], ignore_index = True)\n",
    "df.to_csv('all_word_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f2705a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cm', 'fi', 'gu', 'kk', 'we', 'es', 'ia', 'di', 'ke', 'ya', 'oh', 'ku']\n"
     ]
    }
   ],
   "source": [
    "my_list = list2\n",
    "\n",
    "# Create a new list by separating elements with only two letters\n",
    "new_list = []\n",
    "\n",
    "for item in my_list:\n",
    "    if len(item) == 2:\n",
    "        new_list.append(item)\n",
    "\n",
    "print(new_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
